# AnimateAnyone-Reproduced
## Consistent and Controllable Image-to-Video Synthesis for Character Animation
This repository contains our reproduction of the AnimateAnyone model, a framework for generating high-fidelity, temporally consistent character animations from a single reference image and pose sequences. Our implementation aims to closely replicate the original pipeline while introducing extensions for improved usability, flexibility, and performance. This project is intended for academic research and demonstration purposes.
# 🌟 Features

High-Fidelity Animation: Generates detailed, realistic character animations with consistent appearance across frames.
Pose-Guided Control: Uses pose sequences (e.g., OpenPose keypoints) to drive character movements.
Temporal Consistency: Ensures smooth transitions between frames using advanced temporal modeling.
Extended Features:
Support for multiple pose formats (OpenPose, DensePose, and custom JSON-based keypoints).
Integration with ComfyUI for a streamlined workflow.
Optional text prompt conditioning for stylized animations.
Batch processing for generating multiple videos simultaneously.


Pre-trained Weights: Includes unofficial pre-trained weights adapted from the original model.
Video Demonstrations: Showcases results for various character types (humans, cartoons, humanoid figures).

# 📋 Requirements

Python >= 3.10
CUDA >= 11.7 (for GPU acceleration)
GPU with at least 16GB VRAM (e.g., RTX 3080 or better)
Dependencies listed in requirements.txt

# 🛠 Installation

Clone the Repository:
git clone
cd AnimateAnyone-Reproduced

Set Up Virtual Environment (optional but recommended):
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate


Install Dependencies:
pip install -r requirements.txt

If you encounter issues with the diffusers library, run:
pip install --force-reinstall diffusers>=0.26.1


Download Pre-trained Weights:Run the following command to automatically download weights to the ./pretrained_weights directory:
python scripts/download_weights.py

Alternatively, manually download weights from our HuggingFace page and place them in ./pretrained_weights.


# Our Extension
1. WaterMark added
Our model can add the watermark at the bottom of the video to show this video is generated by our model.
2. Weather Effects
Using apply_weather_effects.py to apply weather effects to your video
You can choose from snowy, sunny, rainy, and foggy to enrich your background.



# 🎥 Video Demonstrations
Below are example videos generated using our model, showcasing its ability to animate diverse characters:

https://github.com/user-attachments/assets/6c9572f5-e991-4c94-a1b7-e6400d160b85



https://github.com/user-attachments/assets/419b311c-37f4-4db2-9736-226aa3a832b2






# 🙏 Acknowledgments

The original AnimateAnyone team for their groundbreaking work.
Moore-AnimateAnyone for inspiration and implementation insights.
HuggingFace for hosting pre-trained weights and demos.

# 📜 License
This project is licensed under the Apache License 2.0. See the LICENSE file for details. Note that this is an unofficial reproduction, and users are responsible for adhering to ethical and legal standards when using the model.
# 🤝 Contributing
We welcome contributions! Please check the CONTRIBUTING.md file for guidelines on submitting issues, pull requests, or new features.
# 📬 Contact
For questions or feedback, open an issue on this repository or contact us at HKU [u3610371@connect.hku.hk].

This project is for academic research purposes only. We disclaim responsibility for user-generated content.

