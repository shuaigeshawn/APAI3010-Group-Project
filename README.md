# AnimateAnyone-Reproduced
## Consistent and Controllable Image-to-Video Synthesis for Character Animation
This repository contains our reproduction of the AnimateAnyone model, a framework for generating high-fidelity, temporally consistent character animations from a single reference image and pose sequences. Our implementation aims to closely replicate the original pipeline while introducing extensions for improved usability, flexibility, and performance. This project is intended for academic research and demonstration purposes.
# üåü Features

High-Fidelity Animation: Generates detailed, realistic character animations with consistent appearance across frames.
Pose-Guided Control: Uses pose sequences (e.g., OpenPose keypoints) to drive character movements.
Temporal Consistency: Ensures smooth transitions between frames using advanced temporal modeling.

Pre-trained Weights: Includes unofficial pre-trained weights adapted from the original model.
Video Demonstrations: Showcases results for various character types (humans, cartoons, humanoid figures).

# üìã Requirements

Python >= 3.10
CUDA >= 11.7 (for GPU acceleration)
GPU with at least 16GB VRAM (e.g., RTX 3080 or better)
Dependencies listed in requirements.txt

# üõ† Installation

Clone the Repository:
git clone
cd AnimateAnyone-Reproduced

Set Up Virtual Environment (optional but recommended):
```shell
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate


#Install Dependencies:
pip install -r requirements.txt

#If you encounter issues with the diffusers library, run:
pip install --force-reinstall diffusers>=0.26.1


#Download Pre-trained Weights:Run the following command to automatically download weights to the ./pretrained_weights directory:
python scripts/download_weights.py

#Alternatively, manually download weights from our HuggingFace page and place them in ./pretrained_weights.
```

# Our Extensions
## Extension 1. WaterMark added

Our model can add the watermark at the bottom of the video to show this video is generated by our model.

## Extension 2. Change Bcakground
   This extension is implementing a post-processing on the video.
   
   Before useing the extension:
   
To demonstrate the work of Animate anyone, the output video of Animate Anyone is concatenated by input image, pose video and animation video. The users will need to crop the animation video out.
```shell 
# We have cropped out two and you can try our extensions with them:
animated.mp4
dancing.mp4
# We also provide two backgrounds:
boom.mp4
danceroom.jpg
```
   
If you want to change the background to a mp4 video, run:
```shell
python mp4_bg.py dancing.mp4 boom.mp4
# You can replace dancing.mp4 with your input video or replace boom.mp4 with another mp4 background
python mp4_bg.py YourAnimation.mp4 YourBackground.mp4
# The output will be YourAnimation_YourBackground_output.mp4
```
If you want to change the background to a picture, run:
```shell
python img_bg.py animated.mp4 danceroom.jpg
# ou can replace animated.mp4 with your input video or replace danceroom.jpg with another picture background
python img_bg.py YourAnimation.mp4 YourBackground.jpg/png
# The output will be YourAnimation_YourBackground_output.mp4
```
If you want to keep the background but stablized, run:
```shell
# Get the background of the original image
python person_background_separation.py bmy.png
# The output will be bmy_person.png and bmy_background.png. Both jpg and png are ok to be the input of this program.
# Then you'll have to use other tools to complete bmy_background.png. After get the complete background, you can run:
python img_bg.py animation.mp4 complete_bmy_background.png  # animation.mp4 is the cropped output video of bmy.png
```
### Limitations and Challenges
The post-processing is extracting the person out of the video. Extractions on the animated person of the post-processed video is not stable. We prepared another script pose2webm.py to replace pose2vid.py in their original program. We hope that with the help of pose2webm.py we can pre-process the input image by running python person_background_separation.py input.png. Then use input_person.png as the input of this modified model to avoid extracting person from the video to make the final result stable. However, the code under this conception didn't run successfully, so the file pose2webm.py is the directory scripts but not included in current pipeline. 

# üé• Video Demonstrations
Below are example videos generated using our model, showcasing its ability to animate diverse characters:

https://github.com/user-attachments/assets/6c9572f5-e991-4c94-a1b7-e6400d160b85



https://github.com/user-attachments/assets/419b311c-37f4-4db2-9736-226aa3a832b2






# üôè Acknowledgments

The original [Animate Anyone](https://humanaigc.github.io/animate-anyone) team for their groundbreaking work.
[Moore-AnimateAnyone](https://github.com/MooreThreads/Moore-AnimateAnyone) for inspiration and implementation insights.
HuggingFace for hosting pre-trained weights and demos.

# üìú License
This project is licensed under the Apache License 2.0. See the LICENSE file for details. Note that this is an unofficial reproduction, and users are responsible for adhering to ethical and legal standards when using the model.
# ü§ù Contributing
We welcome contributions! Please check the CONTRIBUTING.md file for guidelines on submitting issues, pull requests, or new features.
# üì¨ Contact
For questions or feedback, open an issue on this repository or contact us at HKU [u3610371@connect.hku.hk].

This project is for academic research purposes only. We disclaim responsibility for user-generated content.

